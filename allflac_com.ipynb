{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "allflac_com2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpWvss7mJ9ly",
        "colab_type": "text"
      },
      "source": [
        "Program will download from https://allflac.com\n",
        "\n",
        "Instruction:\n",
        "\n",
        "\n",
        "1.   Edit the \"Parameters\"\n",
        "2.   Always run \"Parameters\" and \"1-time Setup\" for every new runtime\n",
        "3.   Run \"Step 1\" until it's done\n",
        "4.   Run \"Step 2\" a few times until there's no error. If there's a prompt to go to google url, click on it and login and copy the verification code and paste it here\n",
        "5.   Run \"Step 3\" a lot of times until it's done. If there's a prompt to go to google url, click on it and login and copy the verification code and paste it here\n",
        "(gdrive limits to 12 hr per run. When it disconnect, reconnect and run Step 3 again, it'll pick up where it left)\n",
        "\n",
        "\n",
        "*Estimated 1 month to completely download everything*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEugRHP0LjZ_",
        "colab_type": "code",
        "outputId": "91c08dc3-10b3-4dfd-8a30-fddb423c88d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#@title Parameters\n",
        "#@markdown Download from https://allflac.com\n",
        "\n",
        "WORKERS = 16 #@param {type:\"number\"}\n",
        "SAVELOCATION = '/gdrive/Shared drives/Team Drive/Downloads' #@param {type:\"string\"}\n",
        "SITEURL='https://allflac.com'\n",
        "\n",
        "import dask\n",
        "dask.config.set(work_stealing=False)\n",
        "dask.config.set(num_workers=WORKERS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeTUABlgrjyo",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title 1-time Setup\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from google.colab import auth, drive\n",
        "import google.auth\n",
        "\n",
        "\n",
        "auth.authenticate_user()\n",
        "credentials, _ = google.auth.default()\n",
        "\n",
        "!pip install gspread-pandas\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtipmAv0Dqjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Step 1 - Generate page number\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from gspread_pandas import Spread\n",
        "\n",
        "\n",
        "def getnumpage():\n",
        "  try:\n",
        "    r = requests.get('%s/albums/artist'%(SITEURL))\n",
        "    soup = BeautifulSoup(r.text, 'html5lib')\n",
        "    lastpage=soup.select('.filter_offset_pages a')[-1].get_text()\n",
        "    return int(lastpage)\n",
        "  except:\n",
        "    return 0\n",
        "\n",
        "spread=Spread('allflac_com2', 'albums_page', create_spread=True, create_sheet=True, creds=credentials)\n",
        "spread.delete_sheet('Sheet1')\n",
        "# spread.clear_sheet()\n",
        "# spread.update_cells('A1', 'B1', ['link', 'done'])\n",
        "\n",
        "urls=['%s/albums/artist?Albums_offset_page=%d'%(SITEURL, i)\n",
        "      for i in range(1, getnumpage()+1)]\n",
        "df=pd.DataFrame(urls, columns=['link'])\n",
        "df['done']=''\n",
        "\n",
        "spread.df_to_sheet(df, index=False, replace=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "code",
        "id": "ABkhNnFNtxaO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "b11d4146-4ae6-4343-da82-4373723ed5c6"
      },
      "source": [
        "#@title Step 2 - Get Albums\n",
        "#@markdown (will take a few hours, run a few time until it is finished and no error)\n",
        "\n",
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from gspread_pandas import Spread\n",
        "from urllib.parse import urljoin\n",
        "from tqdm import tqdm\n",
        "import traceback\n",
        "\n",
        "\n",
        "def cleanup(str):\n",
        "  r = str.rfind(\"'\")\n",
        "  return (str[:r] if r>0 else str).strip()\n",
        "\n",
        "\n",
        "def getalbums(url):\n",
        "  global spread\n",
        "  try:\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.text, 'html5lib')\n",
        "    links=soup.select(\".albums a\")\n",
        "    albums=[[l.img.next_sibling.strip(), cleanup(l.span.text), urljoin(SITEURL,l['href'])] for l in links]\n",
        "    spread.spread.values_append('albums!A1:C1', {'valueInputOption':'RAW'}, {'values':albums})\n",
        "    for cell in spread.sheet.findall(url):\n",
        "      spread.sheet.update_cell(cell.row, 2, \"Y\")\n",
        "  except:\n",
        "    traceback.print_exc()\n",
        "    pass\n",
        "  pbar.update(1)\n",
        "\n",
        "\n",
        "spread=Spread('allflac_com2', 'albums_page', creds=credentials)\n",
        "df1=spread.sheet_to_df(index=None)\n",
        "df1.drop_duplicates('link', keep=False, inplace=True) \n",
        "df2=df1[df1['done']=='']\n",
        "\n",
        "spread.open_sheet('albums', create=True)\n",
        "spread.update_cells('A1', 'D1', ['artist', 'title', 'link', 'done'])\n",
        "spread.open_sheet('albums_page')\n",
        "\n",
        "pbar = tqdm(total=len(df1), ncols=80)\n",
        "pbar.update(len(df1)-len(df2))\n",
        "\n",
        "dd1=dd.from_pandas(df2, npartitions=WORKERS)\n",
        "dd1['link'].apply(getalbums, meta='object').compute()\n",
        "\n",
        "print(\"\\n\\nFound %d albums\"%(spread.find_sheet('albums').row_count-1))\n",
        "\n",
        "#autofit width on current sheet\n",
        "spread.spread.batch_update({\n",
        "    \"requests\": [\n",
        "      {\n",
        "        \"autoResizeDimensions\": {\n",
        "          \"dimensions\": {\n",
        "            \"sheetId\": spread.sheet.id,\n",
        "            \"dimension\": \"COLUMNS\",\n",
        "            \"startIndex\": 0,\n",
        "            \"endIndex\": spread.sheet.col_count\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    ]\n",
        "  })\n",
        "print('Done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZywJUONKEjzL",
        "colab_type": "code",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "5cc24d56-b7a3-4e86-8174-306c293d8220"
      },
      "source": [
        "#@title Step 3 - Download Albums\n",
        "#@markdown (will take very long, maybe a few days, run a few time until it is finished)\n",
        "#@markdown (google colab will stop every 12 hours, just restart and run again,)\n",
        "#@markdown (the program will pickup where it left)\n",
        "\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import urllib.request\n",
        "import shutil\n",
        "from urllib.parse import urljoin\n",
        "from tqdm import tqdm\n",
        "import traceback\n",
        "\n",
        "# import mutagen\n",
        "import requests\n",
        "# import wget\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import dask.dataframe as dd\n",
        "from gspread_pandas import Spread\n",
        "from google.colab import auth, drive\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "\n",
        "def safefilename(name):\n",
        "  return str(name).translate(str.maketrans(r'\\/:*?\"<>|', '_________')).strip().strip('. ')\n",
        "\n",
        "\n",
        "def download_file(url, loc, file_name=None, alt_name=None):\n",
        "  try:\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "      if not file_name:\n",
        "        try:\n",
        "          file_name = response.info().get_filename().encode('latin-1').decode('utf-8')\n",
        "        except:\n",
        "          file_name = alt_name\n",
        "      saveto = os.path.join(loc, os.path.basename(file_name))\n",
        "      #print(\"Saving to: \"+saveto)\n",
        "      if not os.path.exists(saveto):\n",
        "        if not os.path.isdir(loc):\n",
        "            os.makedirs(loc)\n",
        "        with open(saveto, 'wb') as f:\n",
        "          shutil.copyfileobj(response, f, 16*1024*1024)\n",
        "  except:\n",
        "    traceback.print_exc()\n",
        "    pass\n",
        "\n",
        "\n",
        "def download(artist, album, url, songs):\n",
        "  loc = os.path.join(SAVELOCATION, artist, album)\n",
        "  tmploc = os.path.join(\"Downloads\", artist, album)\n",
        "\n",
        "  for i, (title, time) in enumerate(songs):\n",
        "    url2 = urljoin(SITEURL, url + time.split('#')[0])\n",
        "    \n",
        "    if '#' in time:\n",
        "      if not os.path.isdir(loc):\n",
        "          os.makedirs(loc)\n",
        "      tmpfile = time.split('#')[0]+'.flac'\n",
        "      download_file(url2, tmploc, tmpfile)\n",
        "      starttime = time.split('#')[1]\n",
        "      params = ['ffmpeg', \n",
        "                '-hide_banner', '-y', \n",
        "                '-i', os.path.join(tmploc, tmpfile), \n",
        "                '-metadata', 'title=\"'+title.replace('\"','\\\\\"')+'\"',\n",
        "                '-metadata', 'track='+str(i+1),\n",
        "                '-ss', starttime]\n",
        "      try:\n",
        "        endtime = songs[i+1][1].split('#')[1]\n",
        "        params.extend(['-to', endtime])\n",
        "      except:\n",
        "        pass\n",
        "      params.append(os.path.join(loc, '%02d. %s.flac'%(i+1, safefilename(title))))\n",
        "      out=subprocess.run(params, stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
        "    else:\n",
        "      download_file(url2, loc, alt_name='%02d. %s.flac'%(i+1, safefilename(title)))\n",
        "  shutil.rmtree(tmploc, ignore_errors=True)\n",
        "    \n",
        "\n",
        "def downloadAlbum(row):\n",
        "  link = row['link']\n",
        "  page = requests.get(link)\n",
        "  soup = BeautifulSoup(page.text, 'html5lib')\n",
        "  if \"Page not found\" in soup:\n",
        "      return\n",
        "\n",
        "  # get album name and make it safer\n",
        "  album = safefilename(row['title'])\n",
        "  artist = safefilename(row['artist'])\n",
        "\n",
        "  # getting variables\n",
        "  url = soup.find('div', {'id': 'mp3_player'})['data-url'].replace('player', 'flac')\n",
        "\n",
        "  playlist = soup.find('tbody', {'class': 'fp_list'}).find_all('tr')\n",
        "  songs = [[x.text, '.'.join(x.find('a')['data-id'].rsplit(':', 1))] for x in playlist]\n",
        "  #print(songs)\n",
        "  download(artist, album, url, songs)\n",
        "  for cell in spread.sheet.findall(link):\n",
        "    spread.sheet.update_cell(cell.row, 4, \"Y\")\n",
        "  pbar.update(1)\n",
        "         \n",
        "\n",
        "spread=Spread('allflac_com2', 'albums', creds=credentials)\n",
        "df_albums=spread.sheet_to_df(index=None)\n",
        "df_albums.drop_duplicates('link', keep=False, inplace=True)\n",
        "df2=df_albums[df_albums['done']=='']\n",
        "\n",
        "pbar = tqdm(total=len(df_albums), ncols=80)\n",
        "pbar.update(len(df_albums)-len(df2))\n",
        "\n",
        "dd1=dd.from_pandas(df2, npartitions=WORKERS)\n",
        "dd1.apply(downloadAlbum, meta='object', axis=1).compute()\n",
        "\n",
        "\n",
        "print(\"\\n\\nDone\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}